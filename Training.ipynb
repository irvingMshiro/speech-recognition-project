{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "d38bdfad826946c7921222ac5667f772"
          ]
        },
        "id": "v9_FPxKuFmbJ",
        "outputId": "3068f575-9f7b-4766-a36e-0a2c8246b3af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d38bdfad826946c7921222ac5667f772",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6NQDrmmFmbL",
        "outputId": "d6b4193e-bd9b-4109-c045-bbe052f62cd5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Irving\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\datasets\\load.py:1491: FutureWarning: The repository for mozilla-foundation/common_voice_13_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_13_0\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Irving\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\datasets\\load.py:1491: FutureWarning: The repository for mozilla-foundation/common_voice_13_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_13_0\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
            "        num_rows: 5041\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
            "        num_rows: 3292\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\n",
        "    \"mozilla-foundation/common_voice_13_0\", \"id\", split=\"train\"\n",
        ")\n",
        "common_voice[\"validation\"] = load_dataset(\n",
        "    \"mozilla-foundation/common_voice_13_0\", \"id\", split=\"validation\"\n",
        ")\n",
        "\n",
        "print(common_voice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnxrelVNFmbL",
        "outputId": "d40841c7-a17b-4103-8c9c-ff877ba6889c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Irving\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\utils\\hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\n",
        "    \"openai/whisper-small\", language=\"indonesian\", task=\"transcribe\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FAg4RQiFmbL"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1iioSQ7FmbM",
        "outputId": "99ac092f-a275-4281-e7cb-e95f757fa85d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'client_id': Value(dtype='string', id=None),\n",
              " 'path': Value(dtype='string', id=None),\n",
              " 'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\n",
              " 'sentence': Value(dtype='string', id=None),\n",
              " 'up_votes': Value(dtype='int64', id=None),\n",
              " 'down_votes': Value(dtype='int64', id=None),\n",
              " 'age': Value(dtype='string', id=None),\n",
              " 'gender': Value(dtype='string', id=None),\n",
              " 'accent': Value(dtype='string', id=None),\n",
              " 'locale': Value(dtype='string', id=None),\n",
              " 'segment': Value(dtype='string', id=None),\n",
              " 'variant': Value(dtype='string', id=None)}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common_voice[\"train\"].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wwcV-QHMFmbM"
      },
      "outputs": [],
      "source": [
        "from datasets import Audio\n",
        "\n",
        "sampling_rate = processor.feature_extractor.sampling_rate\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AP9om9MhFmbM"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    example = processor(\n",
        "        audio=audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        text=example[\"sentence\"],\n",
        "    )\n",
        "\n",
        "    # compute input length of audio sample in seconds\n",
        "    example[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
        "\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09JyhpC5FmbN"
      },
      "outputs": [],
      "source": [
        "common_voice = common_voice.map(\n",
        "    prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr9UGsJ5FmbN"
      },
      "outputs": [],
      "source": [
        "max_input_length = 30.0\n",
        "\n",
        "def is_audio_in_length_range(length):\n",
        "    return length < max_input_length\n",
        "\n",
        "common_voice[\"train\"] = common_voice[\"train\"].filter(\n",
        "    is_audio_in_length_range,\n",
        "    input_columns=[\"input_length\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyjw0w00FmbN",
        "outputId": "4610721c-9fff-4c96-ee4a-01d1891b3f3b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_features', 'labels', 'input_length'],\n",
              "    num_rows: 5041\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "common_voice[\"train\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agYNON3tFmbN"
      },
      "source": [
        "# Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_4-WLobFmbN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(\n",
        "        self, features: List[Dict[str, Union[List[int], torch.Tensor]]]\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        input_features = [\n",
        "            {\"input_features\": feature[\"input_features\"][0]} for feature in features\n",
        "        ]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(\n",
        "            labels_batch.attention_mask.ne(1), -100\n",
        "        )\n",
        "\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lsF_3v41FmbN"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1215YdP9FmbN"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8qHLBkQHFmbN"
      },
      "outputs": [],
      "source": [
        "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
        "\n",
        "normalizer = BasicTextNormalizer()\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
        "\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = processor.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # compute orthographic wer\n",
        "    wer_ortho = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    # compute normalised WER\n",
        "    pred_str_norm = [normalizer(pred) for pred in pred_str]\n",
        "    label_str_norm = [normalizer(label) for label in label_str]\n",
        "\n",
        "    pred_str_norm = [\n",
        "        pred_str_norm[i] for i in range(len(pred_str_norm)) if len(label_str_norm[i]) > 0\n",
        "    ]\n",
        "    label_str_norm = [\n",
        "        label_str_norm[i]\n",
        "        for i in range(len(label_str_norm))\n",
        "        if len(label_str_norm[i]) > 0\n",
        "    ]\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str_norm, references=label_str_norm)\n",
        "\n",
        "    return {\"wer_ortho\": wer_ortho, \"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anSI2adhFmbO"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJl0vP4MFmbO"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LglQ-ZrMFmbO"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "model.generate = partial(\n",
        "    model.generate, language=\"indonesian\", task=\"transcribe\", use_cache=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlPfNzWlFmbO",
        "outputId": "f29a37fd-4dc5-40a2-c71f-cf2b6f5f2f0e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Irving\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-id-finetuned\",\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,\n",
        "    learning_rate=1e-5,\n",
        "    lr_scheduler_type=\"constant_with_warmup\",\n",
        "    warmup_steps=50,\n",
        "    max_steps=500,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    fp16_full_eval=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=250,\n",
        "    eval_steps=250,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xc9_0RPkFmbO",
        "outputId": "ca299907-b49b-4c11-87e1-1691854f7ac7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "bf44866f4cd84ed6b0ef6a10e121bc9e",
            "2c29096c4af045e8962b7043efb581e8",
            "9b57a3aa803c43d59ec2b9875e37e66a"
          ]
        },
        "id": "Qslj17oUFmbO",
        "outputId": "71932812-077e-431f-d4c0-0ae17d183dd3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf44866f4cd84ed6b0ef6a10e121bc9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Irving\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Irving\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:691: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 3.0424, 'grad_norm': 20.848134994506836, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.08}\n",
            "{'loss': 1.2672, 'grad_norm': 11.023367881774902, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.16}\n",
            "{'loss': 0.7782, 'grad_norm': 10.920741081237793, 'learning_rate': 1e-05, 'epoch': 0.24}\n",
            "{'loss': 0.6292, 'grad_norm': 11.344200134277344, 'learning_rate': 1e-05, 'epoch': 0.32}\n",
            "{'loss': 0.4443, 'grad_norm': 7.063415050506592, 'learning_rate': 1e-05, 'epoch': 0.4}\n",
            "{'loss': 0.3928, 'grad_norm': 6.689120769500732, 'learning_rate': 1e-05, 'epoch': 0.47}\n",
            "{'loss': 0.373, 'grad_norm': 7.373946189880371, 'learning_rate': 1e-05, 'epoch': 0.55}\n",
            "{'loss': 0.3607, 'grad_norm': 5.897298812866211, 'learning_rate': 1e-05, 'epoch': 0.63}\n",
            "{'loss': 0.3749, 'grad_norm': 7.397423267364502, 'learning_rate': 1e-05, 'epoch': 0.71}\n",
            "{'loss': 0.3545, 'grad_norm': 6.90395450592041, 'learning_rate': 1e-05, 'epoch': 0.79}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c29096c4af045e8962b7043efb581e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/206 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3432924449443817, 'eval_wer_ortho': 23.924229315604936, 'eval_wer': 18.66462287543393, 'eval_runtime': 3384.9735, 'eval_samples_per_second': 0.973, 'eval_steps_per_second': 0.061, 'epoch': 0.79}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Irving\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 0.3108, 'grad_norm': 6.03348445892334, 'learning_rate': 1e-05, 'epoch': 0.87}\n",
            "{'loss': 0.375, 'grad_norm': 6.744997978210449, 'learning_rate': 1e-05, 'epoch': 0.95}\n",
            "{'loss': 0.3135, 'grad_norm': 5.01390266418457, 'learning_rate': 1e-05, 'epoch': 1.03}\n",
            "{'loss': 0.1586, 'grad_norm': 4.313182353973389, 'learning_rate': 1e-05, 'epoch': 1.11}\n",
            "{'loss': 0.1395, 'grad_norm': 6.784083843231201, 'learning_rate': 1e-05, 'epoch': 1.19}\n",
            "{'loss': 0.1479, 'grad_norm': 4.548806667327881, 'learning_rate': 1e-05, 'epoch': 1.27}\n",
            "{'loss': 0.1499, 'grad_norm': 4.792871952056885, 'learning_rate': 1e-05, 'epoch': 1.34}\n",
            "{'loss': 0.1692, 'grad_norm': 3.3288323879241943, 'learning_rate': 1e-05, 'epoch': 1.42}\n",
            "{'loss': 0.176, 'grad_norm': 4.489218711853027, 'learning_rate': 1e-05, 'epoch': 1.5}\n",
            "{'loss': 0.1623, 'grad_norm': 3.51639986038208, 'learning_rate': 1e-05, 'epoch': 1.58}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b57a3aa803c43d59ec2b9875e37e66a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/206 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.3319578766822815, 'eval_wer_ortho': 23.163790355630436, 'eval_wer': 18.258870204228845, 'eval_runtime': 4890.3107, 'eval_samples_per_second': 0.673, 'eval_steps_per_second': 0.042, 'epoch': 1.58}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['proj_out.weight'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train_runtime': 15634.2172, 'train_samples_per_second': 0.512, 'train_steps_per_second': 0.032, 'train_loss': 0.5059899759292602, 'epoch': 1.58}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "def find_latest_checkpoint(output_dir):\n",
        "    checkpoints = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "    checkpoints = sorted(checkpoints, key=lambda x: int(x.split(\"-\")[-1]))\n",
        "    return os.path.join(output_dir, checkpoints[-1])\n",
        "\n",
        "checkpoint = find_latest_checkpoint(training_args.output_dir)\n",
        "\n",
        "if checkpoint:\n",
        "    trainer.train(resume_from_checkpoint=checkpoint)\n",
        "else:\n",
        "    trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQgUTVItFmbO",
        "outputId": "e8d4a806-bc81-4b0d-ece3-32e7b87e0d8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n",
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model and the optimizer state\n",
        "trainer.save_model(\"./whisper-id-finetuned/\")\n",
        "z\n",
        "# Save the processor\n",
        "processor.save_pretrained(\"./whisper-id-finetuned/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6TrOEAz4FmbO"
      },
      "outputs": [],
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
        "    \"dataset\": \"Common Voice 13\",\n",
        "    \"language\": \"id\",\n",
        "    \"model_name\": \"Whisper Small Id - Fine Tuned\",\n",
        "    \"finetuned_from\": \"openai/whisper-small\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FnOFF6hSFmbP",
        "outputId": "3e0455ea-dd64-4e1f-b492-630bc618a2c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 448, 'suppress_tokens': [1, 2, 7, 8, 9, 10, 14, 25, 26, 27, 28, 29, 31, 58, 59, 60, 61, 62, 63, 90, 91, 92, 93, 359, 503, 522, 542, 873, 893, 902, 918, 922, 931, 1350, 1853, 1982, 2460, 2627, 3246, 3253, 3268, 3536, 3846, 3961, 4183, 4667, 6585, 6647, 7273, 9061, 9383, 10428, 10929, 11938, 12033, 12331, 12562, 13793, 14157, 14635, 15265, 15618, 16553, 16604, 18362, 18956, 20075, 21675, 22520, 26130, 26161, 26435, 28279, 29464, 31650, 32302, 32470, 36865, 42863, 47425, 49870, 50254, 50258, 50360, 50361, 50362], 'begin_suppress_tokens': [220, 50257]}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/irvingM/whisper-id-finetuned/commit/b29fcbcbd825467c0f8c963513529b29e646341e', commit_message='End of training', commit_description='', oid='b29fcbcbd825467c0f8c963513529b29e646341e', pr_url=None, pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.push_to_hub(**kwargs)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}