{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_n9hsmCF-d5",
        "outputId": "ac5c62f9-c7e9-4732-a905-fd33a76cf327"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Irving\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\utils\\hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "c:\\Users\\Irving\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\datasets\\load.py:1491: FutureWarning: The repository for mozilla-foundation/common_voice_13_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_13_0\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "pipe = pipeline(\n",
        "  \"automatic-speech-recognition\",\n",
        "  model=\"irvingM/whisper-id-finetuned\",\n",
        "  device=device,\n",
        ")\n",
        "\n",
        "ds = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"id\", split=\"test\")\n",
        "sample = ds[19][\"audio\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmTQWwBLF-d7",
        "outputId": "4071db49-32fa-46e7-c789-2c8da9bb7663"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Irving\\anaconda3\\envs\\gpu_env\\lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:691: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ia meminta supaya kecilnya dinaikkan laki.\n"
          ]
        }
      ],
      "source": [
        "prediction = pipe(sample.copy(), batch_size=8)[\"text\"]\n",
        "print(prediction)\n",
        "\n",
        "# we can also return timestamps for the predictions\n",
        "prediction = pipe(sample.copy(), batch_size=8, return_timestamps=True)[\"chunks\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}